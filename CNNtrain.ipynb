{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "from PIL import Image\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 100 images\n",
      "Loading 100 images\n",
      "(400, 400)\n",
      "(400, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "# Loaded a set of images\n",
    "root_dir = \"training/\"\n",
    "\n",
    "image_dir = root_dir + \"images/\"\n",
    "files = os.listdir(image_dir)\n",
    "n = len(files)\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = [load_image(image_dir + files[i]) for i in range(n)]\n",
    "\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "gt_imgs = [load_image(gt_dir + files[i]) for i in range(n)]\n",
    "\n",
    "print(gt_imgs[0].shape)\n",
    "print(imgs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62500, 72, 72, 3)\n",
      "(62500, 72, 72)\n"
     ]
    }
   ],
   "source": [
    "def img_crop_v2(im, w, h, sw, sh):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    for i in range(0,imgheight-h+1,sh):\n",
    "        for j in range(0,imgwidth-w+1,sw):\n",
    "            if is_2d:\n",
    "                im_patch = im[j:j+w, i:i+h]\n",
    "            else:\n",
    "                im_patch = im[j:j+w, i:i+h, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches\n",
    "\n",
    "def pad_image(image, padding):\n",
    "    if (len(image.shape) < 3): # gt image\n",
    "        data = np.pad(image, ((padding, padding), (padding, padding)), 'reflect')\n",
    "    else:\n",
    "        data = np.pad(image, ((padding, padding), (padding, padding), (0, 0)), 'reflect')\n",
    "    return data\n",
    "\n",
    "# Extract patches from input images\n",
    "window_size = 72\n",
    "patch_size = 16 # each patch is 16*16 pixels\n",
    "padding_size = (window_size - patch_size) // 2\n",
    "\n",
    "img_patches = [img_crop_v2(pad_image(imgs[i], padding_size), window_size, window_size, patch_size, patch_size) for i in range(n)]\n",
    "gt_patches = [img_crop_v2(pad_image(gt_imgs[i], padding_size), window_size, window_size, patch_size, patch_size) for i in range(n)]\n",
    "\n",
    "# Linearize list of patches\n",
    "img_patches = np.asarray([img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))])\n",
    "gt_patches =  np.asarray([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])\n",
    "\n",
    "print(img_patches.shape)\n",
    "print(gt_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 128)       9728      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 72, 72, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 36, 36, 256)       819456    \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 36, 36, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 18, 18, 512)       2097664   \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 18, 18, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 41472)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              42468352  \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "re_lu_6 (ReLU)               (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 45,985,922\n",
      "Trainable params: 45,985,922\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, ReLU, Dropout, Flatten, Dense, ELU, Activation\n",
    "from keras.regularizers import l2\n",
    "\n",
    "INPUT_SHAPE=(window_size, window_size, 3)\n",
    "REG = 1e-6\n",
    "\n",
    "# Use Max Pool or a Conv2D Layer with increased stride\n",
    "useMaxPool = True\n",
    "useReLU = True\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=128,\n",
    "                 kernel_size=(5, 5),\n",
    "                 strides=(1, 1),\n",
    "                 padding='same',\n",
    "                 input_shape=INPUT_SHAPE))\n",
    "if useReLU:\n",
    "    model.add(ReLU())\n",
    "else:\n",
    "    model.add(ELU())\n",
    "if useMaxPool:\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),\n",
    "                           strides=None,\n",
    "                           padding='same'))\n",
    "else:\n",
    "    model.add(Conv2D(filters = 128,\n",
    "                     kernel_size = (5, 5),\n",
    "                     strides = (2, 2),\n",
    "                     padding='same',\n",
    "                     input_shape=INPUT_SHAPE))\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Conv2D(filters = 256,\n",
    "                 kernel_size = (5, 5),\n",
    "                 strides = (1, 1),\n",
    "                 padding='same',\n",
    "                 input_shape=INPUT_SHAPE))\n",
    "if useReLU:\n",
    "    model.add(ReLU())\n",
    "else:\n",
    "    model.add(ELU())\n",
    "#model.add(ReLU(max_value=2))\n",
    "if useMaxPool:\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),\n",
    "                           strides=None,\n",
    "                           padding='same'))\n",
    "else:\n",
    "    model.add(Conv2D(filters = 256,\n",
    "                     kernel_size = (4, 4),\n",
    "                     strides = (2, 2),\n",
    "                     padding='same',\n",
    "                     input_shape=INPUT_SHAPE))\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Conv2D(filters = 512,\n",
    "                 kernel_size = (4, 4),\n",
    "                 strides = (1, 1),\n",
    "                 padding='same',\n",
    "                 input_shape=INPUT_SHAPE))\n",
    "if useReLU:\n",
    "    model.add(ReLU())\n",
    "else:\n",
    "    model.add(ELU())\n",
    "#model.add(ReLU(max_value=2))\n",
    "if useMaxPool:\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),\n",
    "                           strides=None,\n",
    "                           padding='same'))\n",
    "else:\n",
    "    model.add(Conv2D(filters = 512,\n",
    "                     kernel_size = (4, 4),\n",
    "                     strides = (2, 2),\n",
    "                     padding='same',\n",
    "                     input_shape=INPUT_SHAPE))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, kernel_regularizer=l2(REG)))\n",
    "if useReLU:\n",
    "    model.add(ReLU())\n",
    "else:\n",
    "    model.add(ELU())\n",
    "#model.add(ReLU(max_value=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(512, kernel_regularizer=l2(REG)))\n",
    "if useReLU:\n",
    "    model.add(ReLU())\n",
    "else:\n",
    "    model.add(ELU())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, kernel_regularizer=l2(REG)))\n",
    "if useReLU:\n",
    "    model.add(ReLU())\n",
    "else:\n",
    "    model.add(ELU())\n",
    "#model.add(ReLU(max_value=2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, kernel_regularizer=l2(REG)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "opt = Adam(lr=0.001) # Adam optimizer with default initial learning rate\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46309\n",
      "16191\n"
     ]
    }
   ],
   "source": [
    "foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "\n",
    "def gt_to_class(gt_patch):\n",
    "    v = np.mean(gt_patch[padding_size:padding_size+patch_size, padding_size:padding_size+patch_size])\n",
    "    df = np.sum(v)\n",
    "    if df > foreground_threshold:\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        return [1, 0]\n",
    "\n",
    "X = np.asarray([ img_patches[i] for i in range(len(img_patches))])\n",
    "Y = np.asarray([gt_to_class(gt_patches[i]) for i in range(len(gt_patches))])\n",
    "print((Y[:, 0] == 1).sum())\n",
    "print((Y[:, 1] == 1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X.shape[0]\n",
    "idx_0 = [idx for idx in range(N) if Y[idx][0] == 1]\n",
    "idx_1 = [idx for idx in range(N) if Y[idx][1] == 1]\n",
    "\n",
    "np.random.shuffle(idx_1)\n",
    "np.random.shuffle(idx_0)\n",
    "L = min(len(idx_0), len(idx_1))\n",
    "\n",
    "newX = np.append(X[idx_0[:L]], X[idx_1[:L]], axis=0)\n",
    "newY = np.append(Y[idx_0[:L]], Y[idx_1[:L]], axis=0)\n",
    "\n",
    "assert(2 * min((Y[:, 0] == 1).sum(), (Y[:, 1] == 1).sum()) == newX.shape[0])\n",
    "assert(2 * min((Y[:, 0] == 1).sum(), (Y[:, 1] == 1).sum()) == newY.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "STEPS_PER_EPOCH = 125\n",
    "BATCH_SIZE = 250\n",
    "EPOCHS = 200\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=15,\n",
    "                             horizontal_flip=True,\n",
    "                             vertical_flip=True)\n",
    "\n",
    "datagen.fit(newX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1012/1012 [==============================] - 1964s 2s/step - loss: 0.2023 - acc: 0.9184\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.20234, saving model to model.h5\n",
      "Epoch 2/200\n",
      "1012/1012 [==============================] - 1962s 2s/step - loss: 0.1838 - acc: 0.9269\n",
      "\n",
      "Epoch 00002: loss improved from 0.20234 to 0.18378, saving model to model.h5\n",
      "Epoch 3/200\n",
      "1012/1012 [==============================] - 1974s 2s/step - loss: 0.1708 - acc: 0.9332\n",
      "\n",
      "Epoch 00003: loss improved from 0.18378 to 0.17078, saving model to model.h5\n",
      "Epoch 4/200\n",
      "1012/1012 [==============================] - 1962s 2s/step - loss: 0.1657 - acc: 0.9364\n",
      "\n",
      "Epoch 00004: loss improved from 0.17078 to 0.16568, saving model to model.h5\n",
      "Epoch 5/200\n",
      " 335/1012 [========>.....................] - ETA: 21:39 - loss: 0.1584 - acc: 0.9396"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "lr_callback = ReduceLROnPlateau(monitor='loss',\n",
    "                                factor=0.2,\n",
    "                                patience=5,\n",
    "                                min_lr=0.001)\n",
    "\n",
    "stop_callback = EarlyStopping(monitor='acc',\n",
    "                              min_delta=0.0001,\n",
    "                              patience=11,\n",
    "                              verbose=1,\n",
    "                              mode='auto')\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint('model.h5',\n",
    "                                      monitor='loss',\n",
    "                                      verbose=1,\n",
    "                                      save_best_only=True,\n",
    "                                      save_weights_only=False,\n",
    "                                      mode='auto',\n",
    "                                      period=1)\n",
    "\n",
    "augment_data = True\n",
    "batch_size = 125\n",
    "augment_data = True\n",
    "samples_per_epoch = X.shape[0]*X.shape[1]*X.shape[2]//256\n",
    "nb_epoch = 200\n",
    "nb_classes = 2\n",
    "\n",
    "def train(X, Y):\n",
    "    def generate_minibatch():\n",
    "        while 1:\n",
    "            # Generate one minibatch\n",
    "            X_batch = np.empty((batch_size, window_size, window_size, 3))\n",
    "            Y_batch = np.empty((batch_size, 2))\n",
    "            for i in range(batch_size):\n",
    "                # Select a random image\n",
    "                idx = np.random.choice(X.shape[0])\n",
    "                shape = X[idx].shape\n",
    "\n",
    "                # Sample a random window from the image\n",
    "                center = np.random.randint(window_size//2, shape[0] - window_size//2, 2)\n",
    "                sub_image = X[idx][center[0]-window_size//2:center[0]+window_size//2,\n",
    "                                   center[1]-window_size//2:center[1]+window_size//2]\n",
    "                gt_sub_image = Y[idx][center[0]-patch_size//2:center[0]+patch_size//2,\n",
    "                                      center[1]-patch_size//2:center[1]+patch_size//2]\n",
    "\n",
    "                # The label does not depend on the image rotation/flip (provided that the rotation is in steps of 90°)\n",
    "                threshold = 0.25\n",
    "                label = (np.array([np.mean(gt_sub_image)]) > threshold) * 1\n",
    "\n",
    "                # Image augmentation\n",
    "                # Random flip\n",
    "                if np.random.choice(2) == 0:\n",
    "                    # Flip vertically\n",
    "                    sub_image = np.flipud(sub_image)\n",
    "                if np.random.choice(2) == 0:\n",
    "                    # Flip horizontally\n",
    "                    sub_image = np.fliplr(sub_image)\n",
    "\n",
    "                # Random rotation in steps of 90°\n",
    "                num_rot = np.random.choice(4)\n",
    "                sub_image = np.rot90(sub_image, num_rot)\n",
    "\n",
    "                label = np_utils.to_categorical(label, nb_classes)\n",
    "                X_batch[i] = sub_image\n",
    "                Y_batch[i] = label\n",
    "\n",
    "            yield (X_batch, Y_batch)\n",
    "    if augment_data:\n",
    "        #model.fit_generator(datagen.flow(X, Y, batch_size=BATCH_SIZE),\n",
    "        #      steps_per_epoch=X.shape[0] // BATCH_SIZE,\n",
    "        #      epochs=EPOCHS,\n",
    "        #      callbacks=[stop_callback, checkpoint_callback],\n",
    "        #      verbose=1)\n",
    "        model.fit_generator(generate_minibatch(),\n",
    "            steps_per_epoch=samples_per_epoch//batch_size//10,\n",
    "            epochs=nb_epoch,\n",
    "            verbose=1,\n",
    "            callbacks=[checkpoint_callback, stop_callback])\n",
    "    else:\n",
    "        model.fit(X, Y, callbacks=[stop_callback, checkpoint_callback])\n",
    "\n",
    "train(np.asarray(imgs), np.asarray(gt_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test_set_images/test_1/test_1.png\n",
      "cnt[0]=971.0\n",
      "cnt[1]=473.0\n",
      "Processing test_set_images/test_2/test_2.png\n",
      "cnt[0]=1138.0\n",
      "cnt[1]=306.0\n",
      "Processing test_set_images/test_3/test_3.png\n",
      "cnt[0]=1095.0\n",
      "cnt[1]=349.0\n",
      "Processing test_set_images/test_4/test_4.png\n",
      "cnt[0]=1181.0\n",
      "cnt[1]=263.0\n",
      "Processing test_set_images/test_5/test_5.png\n",
      "cnt[0]=1097.0\n",
      "cnt[1]=347.0\n",
      "Processing test_set_images/test_6/test_6.png\n",
      "cnt[0]=1076.0\n",
      "cnt[1]=368.0\n",
      "Processing test_set_images/test_7/test_7.png\n",
      "cnt[0]=1106.0\n",
      "cnt[1]=338.0\n",
      "Processing test_set_images/test_8/test_8.png\n",
      "cnt[0]=1070.0\n",
      "cnt[1]=374.0\n",
      "Processing test_set_images/test_9/test_9.png\n",
      "cnt[0]=672.0\n",
      "cnt[1]=772.0\n",
      "Processing test_set_images/test_10/test_10.png\n",
      "cnt[0]=1030.0\n",
      "cnt[1]=414.0\n",
      "Processing test_set_images/test_11/test_11.png\n",
      "cnt[0]=1014.0\n",
      "cnt[1]=430.0\n",
      "Processing test_set_images/test_12/test_12.png\n",
      "cnt[0]=1259.0\n",
      "cnt[1]=185.0\n",
      "Processing test_set_images/test_13/test_13.png\n",
      "cnt[0]=1204.0\n",
      "cnt[1]=240.0\n",
      "Processing test_set_images/test_14/test_14.png\n",
      "cnt[0]=1005.0\n",
      "cnt[1]=439.0\n",
      "Processing test_set_images/test_15/test_15.png\n",
      "cnt[0]=1108.0\n",
      "cnt[1]=336.0\n",
      "Processing test_set_images/test_16/test_16.png\n",
      "cnt[0]=1141.0\n",
      "cnt[1]=303.0\n",
      "Processing test_set_images/test_17/test_17.png\n",
      "cnt[0]=1107.0\n",
      "cnt[1]=337.0\n",
      "Processing test_set_images/test_18/test_18.png\n",
      "cnt[0]=1090.0\n",
      "cnt[1]=354.0\n",
      "Processing test_set_images/test_19/test_19.png\n",
      "cnt[0]=1031.0\n",
      "cnt[1]=413.0\n",
      "Processing test_set_images/test_20/test_20.png\n",
      "cnt[0]=1116.0\n",
      "cnt[1]=328.0\n",
      "Processing test_set_images/test_21/test_21.png\n",
      "cnt[0]=1115.0\n",
      "cnt[1]=329.0\n",
      "Processing test_set_images/test_22/test_22.png\n",
      "cnt[0]=1142.0\n",
      "cnt[1]=302.0\n",
      "Processing test_set_images/test_23/test_23.png\n",
      "cnt[0]=1095.0\n",
      "cnt[1]=349.0\n",
      "Processing test_set_images/test_24/test_24.png\n",
      "cnt[0]=1200.0\n",
      "cnt[1]=244.0\n",
      "Processing test_set_images/test_25/test_25.png\n",
      "cnt[0]=1182.0\n",
      "cnt[1]=262.0\n",
      "Processing test_set_images/test_26/test_26.png\n",
      "cnt[0]=1292.0\n",
      "cnt[1]=152.0\n",
      "Processing test_set_images/test_27/test_27.png\n",
      "cnt[0]=1093.0\n",
      "cnt[1]=351.0\n",
      "Processing test_set_images/test_28/test_28.png\n",
      "cnt[0]=1067.0\n",
      "cnt[1]=377.0\n",
      "Processing test_set_images/test_29/test_29.png\n",
      "cnt[0]=1134.0\n",
      "cnt[1]=310.0\n",
      "Processing test_set_images/test_30/test_30.png\n",
      "cnt[0]=1042.0\n",
      "cnt[1]=402.0\n",
      "Processing test_set_images/test_31/test_31.png\n",
      "cnt[0]=1008.0\n",
      "cnt[1]=436.0\n",
      "Processing test_set_images/test_32/test_32.png\n",
      "cnt[0]=1110.0\n",
      "cnt[1]=334.0\n",
      "Processing test_set_images/test_33/test_33.png\n",
      "cnt[0]=1224.0\n",
      "cnt[1]=220.0\n",
      "Processing test_set_images/test_34/test_34.png\n",
      "cnt[0]=1022.0\n",
      "cnt[1]=422.0\n",
      "Processing test_set_images/test_35/test_35.png\n",
      "cnt[0]=1128.0\n",
      "cnt[1]=316.0\n",
      "Processing test_set_images/test_36/test_36.png\n",
      "cnt[0]=1072.0\n",
      "cnt[1]=372.0\n",
      "Processing test_set_images/test_37/test_37.png\n",
      "cnt[0]=1069.0\n",
      "cnt[1]=375.0\n",
      "Processing test_set_images/test_38/test_38.png\n",
      "cnt[0]=1145.0\n",
      "cnt[1]=299.0\n",
      "Processing test_set_images/test_39/test_39.png\n",
      "cnt[0]=1044.0\n",
      "cnt[1]=400.0\n",
      "Processing test_set_images/test_40/test_40.png\n",
      "cnt[0]=1097.0\n",
      "cnt[1]=347.0\n",
      "Processing test_set_images/test_41/test_41.png\n",
      "cnt[0]=1127.0\n",
      "cnt[1]=317.0\n",
      "Processing test_set_images/test_42/test_42.png\n",
      "cnt[0]=1015.0\n",
      "cnt[1]=429.0\n",
      "Processing test_set_images/test_43/test_43.png\n",
      "cnt[0]=1177.0\n",
      "cnt[1]=267.0\n",
      "Processing test_set_images/test_44/test_44.png\n",
      "cnt[0]=1075.0\n",
      "cnt[1]=369.0\n",
      "Processing test_set_images/test_45/test_45.png\n",
      "cnt[0]=1175.0\n",
      "cnt[1]=269.0\n",
      "Processing test_set_images/test_46/test_46.png\n",
      "cnt[0]=1085.0\n",
      "cnt[1]=359.0\n",
      "Processing test_set_images/test_47/test_47.png\n",
      "cnt[0]=1155.0\n",
      "cnt[1]=289.0\n",
      "Processing test_set_images/test_48/test_48.png\n",
      "cnt[0]=1123.0\n",
      "cnt[1]=321.0\n",
      "Processing test_set_images/test_49/test_49.png\n",
      "cnt[0]=1179.0\n",
      "cnt[1]=265.0\n",
      "Processing test_set_images/test_50/test_50.png\n",
      "cnt[0]=1119.0\n",
      "cnt[1]=325.0\n"
     ]
    }
   ],
   "source": [
    "def predict(X):\n",
    "    patches = img_crop_v2(pad_image(X, padding_size), window_size, window_size, patch_size, patch_size)\n",
    "    img_patches = np.zeros(shape=(len(patches), window_size, window_size, 3))\n",
    "    for index_patch, patch in enumerate(patches):\n",
    "        img_patches[index_patch] = patch\n",
    "    Z = model.predict(img_patches)\n",
    "    Z = (Z[:,0] < Z[:,1]) * 1\n",
    "    Z = Z.reshape(img_patches.shape[0], -1)\n",
    "    return Z\n",
    "\n",
    "def mask_to_submission_strings(model, image_filename):\n",
    "    \"\"\" Reads a single image and outputs the strings that should go into the submission file. \"\"\"\n",
    "    img_number = int(re.search(r\"\\d+\", image_filename).group(0))\n",
    "    Xi = load_image(image_filename)\n",
    "    Zi = predict(Xi)\n",
    "    Zi = Zi.reshape(-1)\n",
    "    patch_size = 16\n",
    "    nb = 0\n",
    "    cnt = np.zeros(2)\n",
    "    print(\"Processing \" + image_filename)\n",
    "    for j in range(0, Xi.shape[1], patch_size):\n",
    "        for i in range(0, Xi.shape[0], patch_size):\n",
    "            label = int(Zi[nb])\n",
    "            cnt[label] += 1\n",
    "            nb += 1\n",
    "            yield(\"{:03d}_{}_{},{}\".format(img_number, j, i, label))\n",
    "    print(\"cnt[0]=\" + str(cnt[0]))\n",
    "    print(\"cnt[1]=\" + str(cnt[1]))\n",
    "\n",
    "def generate_submission(model, submission_filename, image_filenames):\n",
    "    \"\"\" Generate a .csv containing the classification of the test set. \"\"\"\n",
    "    with open(submission_filename, 'w') as f:\n",
    "        f.write('id,prediction\\n')\n",
    "        for fn in image_filenames:\n",
    "            f.writelines(['{}\\n'.format(s) for s in mask_to_submission_strings(model, fn)])\n",
    "\n",
    "from time import time\n",
    "generate_submission(model, f\"cnn_v{int(time())}.csv\", [f'test_set_images/test_{str(i)}/test_{str(i)}.png' for i in range(1, 51)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
